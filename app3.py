import torch
import streamlit as st
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts.prompt import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms.llamacpp import LlamaCpp
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler
from langchain.callbacks import StreamlitCallbackHandler


class StreamHandler(BaseCallbackHandler): ## streamlit data streaming í´ë˜ìŠ¤
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text=initial_text
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text+=token
        self.container.markdown(self.text)

callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
        
@st.cache_resource(show_spinner = False)
def load_model(): ## llm ë¶ˆëŸ¬ì˜¤ê¸°
    MODEL_PATH = "./models/komt-mistral-7b-v1-q4_0.gguf"
    llm = LlamaCpp(        
        model_path = MODEL_PATH,
        temperature = 0,
        max_tokens = 2046,
        n_ctx=2046,
        n_batch=512,
        n_gpu_layers=35,
        top_p = 1,
        callback_manager = callback_manager,
        verbose = True
    )
    return llm

@st.cache_resource(show_spinner = False)
def model_memory(): ## í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    template = """ì•„ë˜ ì£¼ì–´ì§„ ë¬¸ë§¥ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì‹œì˜¤.
    ë¬¸ë§¥ : {context}
    ì§ˆë¬¸ : {question}
    ë‹µë³€ :"""
    prompt = PromptTemplate(input_variables=["context", "question"], template=template)

    return prompt

@st.cache_resource(show_spinner = False)
def get_vectorstore(): ## ì„ë² ë”© ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°(í¬ë¡œë§ˆdb)
    model_name = "./models/ko-sbert-nli"
    EMBEDDINGS = HuggingFaceEmbeddings(model_name=model_name,
                                model_kwargs={'device':"cuda"},
                                encode_kwargs = {'normalize_embeddings' : True})
    PERSIST_DIRECTORY = "D:\python\FIND_INFO_IN_PDF\chroma_db"
    DB_Chroma = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=EMBEDDINGS)
    vectorstore = DB_Chroma.as_retriever(search_kwargs = {'k':1})
    return vectorstore

@st.cache_resource(show_spinner = False)
def qa_retriever(): ## qa retriever
    prompt = model_memory()
    QA = RetrievalQA.from_chain_type(
        llm=load_model(),
        chain_type="stuff",
        retriever=get_vectorstore(),
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt},
    )
    return QA

def main():
    st.set_page_config(page_title="AI ì±—ë´‡ì—ê²Œ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš” ", page_icon=":books:") ## ì‚¬ì´ë“œ ì½˜í…ì¸  ë¶€ë¶„
    with st.sidebar:
        st.title("ğŸ˜ƒ ì‚¬ë‚´ ì—…ë¬´ì ˆì°¨ ê²€ìƒ‰ CHAT AI ì‹œìŠ¤í…œ ì…ë‹ˆë‹¤.ğŸ’¬")
        st.markdown("ì‚¬ë‚´í‘œì¤€ ë“± ì—…ë¬´ì™€ ê´€ë ¨ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì‚¬ì „ í•™ìŠµí•˜ì—¬ AIê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” CHAT MODEL ì…ë‹ˆë‹¤.")
        st.write("Made by â¤ï¸")
    st.title("AI ì±—ë´‡ì—ê²Œ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš” ğŸ’¬")
    if "messages" not in st.session_state.keys():
        st.session_state.messages = [{"role": "assistant", "content": "ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”! ì—…ë¬´ì ˆì°¨ ê²€ìƒ‰ AI BOT ì…ë‹ˆë‹¤."}]
    # Display or clear chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])
            
    def clear_chat_history(): ## ì±„íŒ… íˆìŠ¤í† ë¦¬ ì‚­ì œ(ì‚¬ì´ë“œ)
        st.session_state.messages = [{"role": "assistant", "content": "ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”! ì—…ë¬´ì ˆì°¨ ê²€ìƒ‰ AI BOT ì…ë‹ˆë‹¤."}]
    st.sidebar.button("ì±„íŒ…ë‚´ìš© ì§€ìš°ê¸°", on_click=clear_chat_history)

    if question := st.chat_input("ê¶ê¸ˆí•˜ì‹  ì—…ë¬´ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"): ## 4-2 ì‚¬ìš©ì ì…ë ¥ ë°›ëŠ” ë¶€ë¶„
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.write(question)

    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                qa = qa_retriever()
                chat_box = st.empty()
                stream_hander = StreamHandler(chat_box)
                response = qa(question, callbacks=[stream_hander])
                answer, docs = response["result"], response["source_documents"][0]
                docs_metadata = docs.metadata['source']
                message = {"role": "assistant", "content": answer}
                st.session_state.messages.append(message)

if __name__ == '__main__':
    main()
